{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75097a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from model import Model\n",
    "from ploter import Ploter\n",
    "from backup_manager import BackupManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c629655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#files : 203\n",
      "files : ['data0.pt', 'data1.pt', 'data10.pt', 'data100.pt', 'data101.pt', 'data102.pt', 'data103.pt', 'data104.pt', 'data105.pt', 'data106.pt', 'data107.pt', 'data108.pt', 'data109.pt', 'data11.pt', 'data110.pt', 'data111.pt', 'data112.pt', 'data113.pt', 'data114.pt', 'data115.pt', 'data116.pt', 'data117.pt', 'data118.pt', 'data119.pt', 'data12.pt', 'data120.pt', 'data121.pt', 'data122.pt', 'data123.pt', 'data124.pt', 'data125.pt', 'data126.pt', 'data127.pt', 'data128.pt', 'data129.pt', 'data13.pt', 'data130.pt', 'data131.pt', 'data132.pt', 'data133.pt', 'data134.pt', 'data135.pt', 'data136.pt', 'data137.pt', 'data138.pt', 'data139.pt', 'data14.pt', 'data140.pt', 'data141.pt', 'data142.pt', 'data143.pt', 'data144.pt', 'data145.pt', 'data146.pt', 'data147.pt', 'data148.pt', 'data149.pt', 'data15.pt', 'data150.pt', 'data151.pt', 'data152.pt', 'data153.pt', 'data154.pt', 'data155.pt', 'data156.pt', 'data157.pt', 'data158.pt', 'data159.pt', 'data16.pt', 'data160.pt', 'data161.pt', 'data162.pt', 'data163.pt', 'data164.pt', 'data165.pt', 'data166.pt', 'data167.pt', 'data168.pt', 'data169.pt', 'data17.pt', 'data170.pt', 'data171.pt', 'data172.pt', 'data173.pt', 'data174.pt', 'data175.pt', 'data176.pt', 'data177.pt', 'data178.pt', 'data179.pt', 'data18.pt', 'data180.pt', 'data181.pt', 'data182.pt', 'data183.pt', 'data184.pt', 'data185.pt', 'data186.pt', 'data187.pt', 'data188.pt', 'data189.pt', 'data19.pt', 'data190.pt', 'data191.pt', 'data192.pt', 'data193.pt', 'data194.pt', 'data195.pt', 'data196.pt', 'data197.pt', 'data198.pt', 'data199.pt', 'data2.pt', 'data20.pt', 'data200.pt', 'data201.pt', 'data202.pt', 'data21.pt', 'data22.pt', 'data23.pt', 'data24.pt', 'data25.pt', 'data26.pt', 'data27.pt', 'data28.pt', 'data29.pt', 'data3.pt', 'data30.pt', 'data31.pt', 'data32.pt', 'data33.pt', 'data34.pt', 'data35.pt', 'data36.pt', 'data37.pt', 'data38.pt', 'data39.pt', 'data4.pt', 'data40.pt', 'data41.pt', 'data42.pt', 'data43.pt', 'data44.pt', 'data45.pt', 'data46.pt', 'data47.pt', 'data48.pt', 'data49.pt', 'data5.pt', 'data50.pt', 'data51.pt', 'data52.pt', 'data53.pt', 'data54.pt', 'data55.pt', 'data56.pt', 'data57.pt', 'data58.pt', 'data59.pt', 'data6.pt', 'data60.pt', 'data61.pt', 'data62.pt', 'data63.pt', 'data64.pt', 'data65.pt', 'data66.pt', 'data67.pt', 'data68.pt', 'data69.pt', 'data7.pt', 'data70.pt', 'data71.pt', 'data72.pt', 'data73.pt', 'data74.pt', 'data75.pt', 'data76.pt', 'data77.pt', 'data78.pt', 'data79.pt', 'data8.pt', 'data80.pt', 'data81.pt', 'data82.pt', 'data83.pt', 'data84.pt', 'data85.pt', 'data86.pt', 'data87.pt', 'data88.pt', 'data89.pt', 'data9.pt', 'data90.pt', 'data91.pt', 'data92.pt', 'data93.pt', 'data94.pt', 'data95.pt', 'data96.pt', 'data97.pt', 'data98.pt', 'data99.pt']\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed54e8989afc4030bfb0cdce56b2c9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading files:   0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data shape: torch.Size([101500, 1000, 4])\n",
      "Merged parameters shape: torch.Size([101500, 1])\n"
     ]
    }
   ],
   "source": [
    "model, data, parameters, raw_data, raw_parameters = BackupManager.load_data_and_build_model(\"data\", stride=10, pre_N=200, preruns=2, seed=42)\n",
    "n_samples = data.shape[0]\n",
    "n_points = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9756929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 1"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m epochs = \u001b[32m200\u001b[39m\n\u001b[32m      3\u001b[39m save_every = \u001b[32m30\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msave_every\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m*save_every,epochs,save_every): \n\u001b[32m      6\u001b[39m     model.resume_training(max_num_epochs=epoch-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/project/sbi-particle-physics/model.py:103\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, data, parameters, stop_after_epochs, max_num_epochs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.neural_network.append_simulations(parameters, data)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(stop_after_epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;28mself\u001b[39m.neural_network.train(stop_after_epochs=stop_after_epochs)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(max_num_epochs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mneural_network\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_num_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.posterior = \u001b[38;5;28mself\u001b[39m.neural_network.build_posterior(sample_with=\u001b[33m'\u001b[39m\u001b[33mrejection\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/sbi/inference/trainers/npe/npe_c.py:203\u001b[39m, in \u001b[36mNPE_C.train\u001b[39m\u001b[34m(self, num_atoms, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, use_combined_loss, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_non_atomic_loss:\n\u001b[32m    200\u001b[39m         \u001b[38;5;66;03m# Take care of z-scoring, pre-compute and store prior terms.\u001b[39;00m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28mself\u001b[39m._set_state_for_mog_proposal()\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/sbi/inference/trainers/npe/npe_base.py:419\u001b[39m, in \u001b[36mPosteriorEstimatorTrainer.train\u001b[39m\u001b[34m(self, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m         theta_batch, x_batch, masks_batch = (\n\u001b[32m    414\u001b[39m             batch[\u001b[32m0\u001b[39m].to(\u001b[38;5;28mself\u001b[39m._device),\n\u001b[32m    415\u001b[39m             batch[\u001b[32m1\u001b[39m].to(\u001b[38;5;28mself\u001b[39m._device),\n\u001b[32m    416\u001b[39m             batch[\u001b[32m2\u001b[39m].to(\u001b[38;5;28mself\u001b[39m._device),\n\u001b[32m    417\u001b[39m         )\n\u001b[32m    418\u001b[39m         \u001b[38;5;66;03m# Take negative loss here to get validation log_prob.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m         val_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtheta_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m            \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmasks_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproposal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcalibration_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_first_round_loss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_first_round_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    427\u001b[39m         val_loss_sum += val_losses.sum().item()\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Take mean over all validation samples.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/sbi/inference/trainers/npe/npe_base.py:647\u001b[39m, in \u001b[36mPosteriorEstimatorTrainer._loss\u001b[39m\u001b[34m(self, theta, x, masks, proposal, calibration_kernel, force_first_round_loss)\u001b[39m\n\u001b[32m    645\u001b[39m     x = reshape_to_batch_event(x, event_shape=\u001b[38;5;28mself\u001b[39m._neural_net.condition_shape)\n\u001b[32m    646\u001b[39m     \u001b[38;5;66;03m# Use posterior log prob (without proposal correction) for first round.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_neural_net\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# Currently only works for `DensityEstimator` objects.\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;66;03m# Must be extended ones other Estimators are implemented. See #966,\u001b[39;00m\n\u001b[32m    651\u001b[39m     loss = -\u001b[38;5;28mself\u001b[39m._log_prob_proposal_posterior(theta, x, masks, proposal)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/sbi/neural_nets/estimators/nflows_flow.py:122\u001b[39m, in \u001b[36mNFlowsFlow.loss\u001b[39m\u001b[34m(self, input, condition)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, condition: Tensor) -> Tensor:\n\u001b[32m    113\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return the negative log-probability for training the density estimator.\u001b[39;00m\n\u001b[32m    114\u001b[39m \n\u001b[32m    115\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m \u001b[33;03m        Negative log-probability of shape `(batch_dim,)`.\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/sbi/neural_nets/estimators/nflows_flow.py:109\u001b[39m, in \u001b[36mNFlowsFlow.log_prob\u001b[39m\u001b[34m(self, input, condition)\u001b[39m\n\u001b[32m    106\u001b[39m ones_for_event_dims = (\u001b[32m1\u001b[39m,) * condition_event_dims  \u001b[38;5;66;03m# Tuple of 1s, e.g. (1, 1, 1)\u001b[39;00m\n\u001b[32m    107\u001b[39m condition = condition.repeat(input_sample_dim, *ones_for_event_dims)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m log_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m log_probs.reshape((input_sample_dim, input_batch_dim))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/nflows/distributions/base.py:40\u001b[39m, in \u001b[36mDistribution.log_prob\u001b[39m\u001b[34m(self, inputs, context)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inputs.shape[\u001b[32m0\u001b[39m] != context.shape[\u001b[32m0\u001b[39m]:\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     38\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of input items must be equal to number of context items.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/nflows/flows/base.py:39\u001b[39m, in \u001b[36mFlow._log_prob\u001b[39m\u001b[34m(self, inputs, context)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_log_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context):\n\u001b[32m     38\u001b[39m     embedded_context = \u001b[38;5;28mself\u001b[39m._embedding_net(context)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     noise, logabsdet = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedded_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     log_prob = \u001b[38;5;28mself\u001b[39m._distribution.log_prob(noise, context=embedded_context)\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m log_prob + logabsdet\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/nflows/transforms/base.py:56\u001b[39m, in \u001b[36mCompositeTransform.forward\u001b[39m\u001b[34m(self, inputs, context)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, context=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     55\u001b[39m     funcs = \u001b[38;5;28mself\u001b[39m._transforms\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cascade\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuncs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/nflows/transforms/base.py:50\u001b[39m, in \u001b[36mCompositeTransform._cascade\u001b[39m\u001b[34m(inputs, funcs, context)\u001b[39m\n\u001b[32m     48\u001b[39m total_logabsdet = inputs.new_zeros(batch_size)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m funcs:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     outputs, logabsdet = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     total_logabsdet += logabsdet\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, total_logabsdet\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/nflows/transforms/coupling.py:83\u001b[39m, in \u001b[36mCouplingTransform.forward\u001b[39m\u001b[34m(self, inputs, context)\u001b[39m\n\u001b[32m     80\u001b[39m identity_split = inputs[:, \u001b[38;5;28mself\u001b[39m.identity_features, ...]\n\u001b[32m     81\u001b[39m transform_split = inputs[:, \u001b[38;5;28mself\u001b[39m.transform_features, ...]\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m transform_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43midentity_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m transform_split, logabsdet = \u001b[38;5;28mself\u001b[39m._coupling_transform_forward(\n\u001b[32m     85\u001b[39m     inputs=transform_split, transform_params=transform_params\n\u001b[32m     86\u001b[39m )\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unconditional_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/sbi/neural_nets/net_builders/flow.py:1370\u001b[39m, in \u001b[36mContextSplineMap.__call__\u001b[39m\u001b[34m(self, inputs, context, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, context: Tensor, *args, **kwargs) -> Tensor:\n\u001b[32m   1359\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1360\u001b[39m \u001b[33;03m    Return parameters of the spline given the context.\u001b[39;00m\n\u001b[32m   1361\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1368\u001b[39m \u001b[33;03m        Spline parameters.\u001b[39;00m\n\u001b[32m   1369\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspline_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mlhep/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "id = \"2\"\n",
    "epochs = 200\n",
    "save_every = 30\n",
    "model.train(data, parameters, max_num_epochs=save_every-1)\n",
    "for epoch in range(2*save_every,epochs,save_every): \n",
    "    model.resume_training(max_num_epochs=epoch-1)\n",
    "    BackupManager.save_model(model, f\"models/training_{id}/epoch_{epoch}.pkl\")\n",
    "Ploter.plot_loss(model.neural_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_parameter, observed_sample = model.get_random_true_parameter(n_points)\n",
    "print(\"True parameter\", true_parameter)\n",
    "n_sampled_parameters = 1000\n",
    "sampled_parameters = model.draw_parameters_from_predicted_posterior(observed_sample, n_sampled_parameters)\n",
    "print(\"Predicted parameter mean\", sampled_parameters.mean())\n",
    "Ploter.plot_a_posterior(sampled_parameters, true_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c627010",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_similar = 5\n",
    "Ploter.plot_similar_data(model, observed_sample, n_similar, n_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd0d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "BackupManager.save_model(model, f\"models/model_{id}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlhep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
